---
title: "Quick Demo"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Machine Readable Hypothesis Tests}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(scienceverse)
```

One goal of scienceverse is to automate the evaluation of predictions. A researcher specifies the prediction in the preregistration, collects the data, and scienceverse can then take the preregistration file and the data and automatically evaluate whether the predictions were confirmed or not.

The demo below follows the example from: 

Lakens, D., & DeBruine, L. M. (2020, January 27). Improving Transparency, Falsifiability, and Rigour by Making Hypothesis Tests Machine Readable. <https://doi.org/10.31234/osf.io/5xcda>

### study

Set up the study with a name and any additional info you want to add.

```{r}
study <- study(name = "Kinship and Prosocial Behaviour",
               abstract = "A reanalysis of data from DeBruine (2002) Facial Resemblance Enhances Trust, PRSLB.")
```

### add_author

Add each author in the following format. Use the function `credit_roles()` to see a list of the roles and their descriptions. Use `credit_roles("name")` or `credit_roles("abbr")` to just see their names or abbreviations. You can also add any further info, like an orcid or email address.

```{r}
study <- add_author(study, 
                    orcid = "0000-0002-7523-5539",
                    surname = "DeBruine",
                    given = "Lisa M.",
                    roles = c("con", "dat", "sof", "dra", "edi"),
                    email = "lisa.debruine@glasgow.ac.uk") %>%
         add_author(orcid = "0000-0002-0247-239X",
                    surname = "Lakens",
                    given = "DaniÃ«l",
                    roles = c("con", "ana", "dra", "edi"))
```

### add_hypothesis

Now add a hypothesis with a verbal description. You can add more than one hypothesis, but this demo study only has one.

```{r}
study <- add_hypothesis(study, id = "self_pref",
                        description = "Cues of kinship will increase prosocial behaviour. Cues of kinship will be manipulated by morphed facial self-resemblance. Prosocial behaviour will be measured by responses in the trust game. The prediction is that the number of trusting AND/OR reciprocating moves will be greater to self morphs than to other morphs.")
```

### add_analysis

Add all relevant analyses for testing this hypothesis in the order that they should be run. Do any data prep first. Each analysis needs an `id` for reference in the criteria later. The `code` can be R code (wrap it in {} if you need more than one function) or the file path for a .R file. You can also add other information about your analysis, such as the software it's running on. 

```{r}
# add by writing the code directly into this function
study <- add_analysis(study,
                      id = "trust",
                      code = t.test(kin$trust_self, 
                                    kin$trust_other, 
                                    paired = TRUE, 
                                    conf.level = 0.975), 
                      software = R.version.string)

# add by referencing an external .R file
study <- add_analysis(study,
                      id = "recip",
                      code = "files/recip_analysis.R", 
                      software = R.version.string)
```

### add_criterion

Next, add the criteria you will need to check to corroborate or falsify each hypothesis. Each criterion needs a unique `id` so you can reference it next in the evaluation. Then, specify the `analysis_id` where you'll look for the result. For example, the analysis `trust_analysis` returns a list from the function `t.test`, which includes a value for `conf.int`. To see if the first number in this vector is larger than 0, set `result` to "conf.int[1]", `operator` to ">" , and `comparator` to $0$. The options for `operator` are ">", "<", "=", and "!=".

```{r}
study <- study %>%
  add_criterion(id = "trust_lowbound",
                analysis_id = "trust",
                result = "conf.int[1]",
                operator = ">",
                comparator = 0) %>%
  add_criterion(id = "trust_highbound",
                analysis_id = "trust",
                result = "conf.int[2]",
                operator = ">",
                comparator = 0.2) %>%
  add_criterion(id = "recip_lowbound",
                analysis_id = "recip",
                result = "conf.int[1]",
                operator = ">",
                comparator = 0) %>%
  add_criterion(id = "recip_highbound",
                analysis_id = "recip",
                result = "conf.int[2]",
                operator = ">",
                comparator = 0.2)
  
```

### add_eval

Add evaluation criteria for corroboration and falsification. Include a verbal `description` and a logical `evaluation` that references the criterion `id`s above.

```{r}
study <- add_eval(study, "corroboration",
                  description = "The hypothesis is corroborated if the 97.5% CI lower bound is greater than 0 and the 97.5% CI upper bound is greater than 0.2 (the SESOI) for either the trust or reciprocation moves.",
                  evaluation = "(trust_lowbound & trust_highbound) | (recip_lowbound & recip_highbound)") %>%
         add_eval("falsification",
                  description = "The hypothesis is falsified if the 97.5% CI upper bound is smaller than 0.2 (the SESOI) for both trust and reciprocation.",
                  evaluation = "!trust_highbound & !recip_highbound")
```

### study_save

At this point, you can save your study to a JSON-formatted file.

```{r}
study_save(study, "files/prereg.json")
```

You can view the [JSON-formatted study description](files/prereg.json) here.

### study_report prereg

You can also export a human-readable pre-registration using the built-in "prereg" template.

```{r}
study_report(study, "prereg", "files/prereg.html")
```

View the [prereg](files/prereg.html) report here.

### add_data

The `id` you give the data can be used in the analysis code to refer to this dataset. The `data` argument can be a data frame or a file path to a data file or a PsychDS-formatted codebook. You can optionally add column parameters such as descriptions with the argument `vardesc`. 

```{r}

data <- data.frame(
  trust_self  = c(1,2,2,1,1,1,1,1,2,0,2,0,1,2,2,3,2,2,1,1,2,0,0,1),
  trust_other = c(1,2,2,0,1,0,0,0,1,0,1,0,1,1,1,0,1,2,2,0,0,0,2,1),
  recip_self  = c(0,1,3,2,1,1,1,3,3,2,3,1,1,2,3,3,3,1,1,1,3,0,3,1),
  recip_other = c(1,1,2,2,3,2,1,3,3,1,3,0,1,3,3,3,3,0,3,0,1,0,3,2)
)

vardesc <- list(
  description = list(
    trust_self  = "Number of trusting moves towards self-morphs",
    trust_other = "Number of trusting moves towards self-morphs",
    recip_self  = "Number of reciprocating moves towards other-morphs",
    recip_other = "Number of reciprocating moves towards other-morphs"
  )
)

study <- add_data(study, id = "kin", data = data, vardesc = vardesc)

```

### study_analyse

When you run `study_analyse`, the data are loaded as their `id` names and the analyses are run in order. The evaluation of each criterion and hypothesis is printed as a message.

```{r}
study <- study_analyse(study)
```


### study_report postreg

At this point, you can save the post-registration version of your study to a JSON-formatted file and also export a human-readable post-registration report.

```{r}
study_save(study, "files/postreg.json")
study_report(study, "postreg", "files/postreg.html")
```

View the [postreg](files/postreg.html) report here.

### Full JSON format

You can view the [JSON-formatted study description](files/postreg.json) here.

